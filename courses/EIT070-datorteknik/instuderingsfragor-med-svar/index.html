<!DOCTYPE html>
<html class="no-js">
  <head>
    <meta charset="utf-8"/>
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
    <title>Datateknik LTH</title>
    <meta name="description"/>
    <meta content="width=device-width" name="viewport"/>
    <link href="../../../stylesheets/application-c673fa9e.css" rel="stylesheet" type="text/css" />
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../javascripts/application-7044a308.js" type="text/javascript"></script>
  </head>
  <body>
    <header>
  <div class="inner">
    <img src="/images/dseklogo-4550b4c2.svg">
    <h1>Datateknik LTH</h1>
    <a href="https://github.com/datateknik-lth/datateknik-lth" class="button">
      <img src="/images/github-5740f0d2.png">
      <small>Visa på</small>GitHub
    </a>

    <h2><a class="nav-link" href="../../../">Kurser</a> / <a class="nav-link" href="../">EIT070-datorteknik</a> / instuderingsfragor-med-svar</h2>
  </div>
</header>


    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <p><strong>Varning:</strong> Dessa svar är skrivna av studenter och är därför inte garanterat rätt och absolut inte förklarat på bästa sätt, förbättringar välkomnas!</p>

<h2>Processorn</h2>

<ol>
<li><p>Vad  är  Moores  lag?</p>

<p>Moores lag är observationen att antalet transistorer i en integrerad krets dubbleras ungefär vartannat år.</p></li>
<li><p>Vem  är  Von  Neumann?</p>

<p>John von Neumann var en matematiker som bland annat beskrev den datorarkitektur som lade grunden till dagens moderna datorarkitekturer. Den datorarkitekturen, Von-Neumann arkitekturen, gick ut på att se program och datan som programmet använder, som samma sak och placera de i samma minne. Här är ett diagram som visar Von-Neumann arkitektur.</p>

<p><img alt="Diagram på Von-Neumann arkitektur" src="../bilder/von_newmann_architecture-05c622ff.jpg" /></p>

<p><a href="http://www.teach-ict.com/as_as_computing/ocr/H447/F453/3_3_3/vonn_neuman/miniweb/pg3.htm">(Källa)</a></p></li>
<li><p>Vad  gör  en  kompilator?</p>

<p>En kompilator översätter ett högnivå programspråk till ett annat med lägre nivå, ofta assembly eller direkt till maskinkod.</p></li>
<li><p>Vad  gör  en  assemblator?</p>

<p>Assemblerar assemblerkod till maskinkod.</p></li>
<li><p>Ge  exempel  på  högnivåspråk?</p>

<p>C och C++ är de vanligaste språken som kompilerar till assembler eller maskinkod medans Java är ett annat exempel som kompilerar till bytekod som sedan körs av en virtuell maskin (i detta fall JVM).</p></li>
<li><p>Vad  skiljer  ett  högnivåspråk  från  ett  maskinspråk?</p>

<p>Datorn kan inte direkt läsa ett högnivåspråk utan det måste genomgå kompilation till maskinspråk (eller bytekod om det ska köras i t.ex. JVM) för att kunna exekveras. Det är också lättare att producera kod säkert och lättförståerligt i högnivåspråk, medans assembly-kod tenderar att vara jobbigt att både utveckla och förstå/läsa.</p></li>
<li><p>Görs  alla  beräkningar  (+, -,  ...,  AND,  OR)  i  ALU:n?</p>

<p>Nej, inte de som involverar flyttal.</p></li>
<li><p>Ge  exempel  på  indata  och  utdata  till  en  kontrollenhet</p></li>
<li><p>Ge  exempel  på  fördelar  med  att  använda  register  för  att  lagra  data</p>

<p>Data i register tar mindre tid att hämta än data i t.ex. CPU cache, RAM-minne eller från disk. Det går även snabbare att spara till register. I en pipelinearkitektur så kan dessutom register användas av flera instruktioner samtidigt, vilket inte är möjligt med t.ex. primärminne.</p></li>
<li><p>Om  en  processor  gör  ”Fetch”  och  ”Execute”,  vad  görs  under  ”Fetch?  Vad  görs  under  ”Execute”?  Är  det  som  görs  under  ”Fetch”  samma  för  alla  instruktioner”</p></li>
</ol>

<h2>Pipelining</h2>

<ol>
<li><p>Vad  är  pipelining?</p>

<p>Pipelining är när processorn kör flera olika instruktioner samtidigt genom att tillåta en instruktion i varje fas (t.ex en i &quot;Fetch&quot; och en i &quot;Execute&quot;).</p></li>
<li><p>Vilka  konflikter  kan  uppstå  i  en  pipeline?  </p>

<p><strong>Strukturella konflikter</strong></p>

<p>När en två pipelineade instruktioner försöker använda en hårdvarukomponent (ex. primärminnet) samtidigt</p>

<p><strong>Datakonflikter</strong></p>

<p>När en instruktion beror på data som påverkas av en exekverande instruktion (Uppstår när en instruktion väntar på resultaten från en annan instruktion)
Penalty kan minskas med hjälp av forwarding/bypassing.
Instruktionerna kan även omordnas så att andra instruktioner körs medans den beroende instruktionen ändå bara hade väntat på svar.</p>

<p><strong>Kontrollkonflikter</strong></p>

<p>När en instruktion som låg direkt efter en branch instruktion påbörjat exekvering, men som senare visar sig <em>inte</em> ska exekveras. Alltså då om branchen genomfördes.</p></li>
<li><p>Illustrera  hur  konflikter  uppstår?</p>

<p>Denna bilden är hämtad från pipelining föreläsningen.</p>

<p><img alt="Illustration av hur kontrollkonflikter kan uppstå" src="../bilder/kontrollhazaradsPipeline-429af7db.png" /></p></li>
<li><p>Vad  kan  man  göra  för  att  undvika  konflikter? </p>

<p>Generell åtgärd är att sätta stalls, eller &#39;nop&#39;-instruktioner, efter varje instruktion som kan orsaka en pipelinekonflikt.
En annan åtgärd, inriktat mot att förhindra kontrollkonflikter, är att använda sig av delayed branching (förklaras nedan.)</p></li>
<li><p>Vad  är  branchpredikion? </p>

<p>När processorn försöker, med hjälp av en predictionalgoritm, gissa vilken väg en kommande branch-instruktion kommer ta. Det finns två generella strategier för detta:</p>

<p><strong>Alternativ förklaring:</strong> Ett sätt att undvika control hazards genom att förutse om en conditional branch-instruktion kommer att leda till branch eller ej och börjar exekvera nästkommande spekulerade instruktion, så kallad spekulativ exekvering.</p>

<p><strong>Statisk prediktion</strong>, där man <em>inte</em> tar hänsyn till historiken. Olika implementeringar:</p></li>
</ol>

<pre><code>- Predict never taken – antar att hoppet inte kommer
</code></pre>

<p>tas (Motorola 68020)
    - Predict always taken – antar att hoppet alltid kommer tas
    - Predict beroende på riktning (Power PC 601):
      - Predict branch taken för tillbaka hopp
      - Predict branch not taken för framåt hopp&#39;</p>

<pre><code>**Dynamisk prediktion**, där man tar hänsyn till historiken:

- 1-bit prediktering: Man sparar vad som hände vid förra branchen, och antar att samma kommer hända vid nästa branch-instruktion.
- 2-bit prediktering: Man använder en state-machine, vilket är lättast förstått av att titta på diagrammet nedan, som är hämtat från Pipelining föreläsningen.

![Diagram över en 2-bit prediction state machine](./bilder/prediction2bit.png)
</code></pre>

<ol>
<li><p>Vad  är  spekulativ  exekvering?</p>

<p>När processorn börjar exekvera instruktioner baserat på branch-gissningen gjord av en branch-prediction.</p></li>
<li><p>Delayed  branching  – vad  är  det?  Ge  ett  exempel.</p>

<p>När instruktioner körda innan en branch-instruktion, som inte kommer påverka om branchen kommer genomgöras eller ej, läggs precis efter en branch-instruktion så stall/nop inte behövs.</p>

<p>Ett exempel <em>innan</em> delayed branching:</p>

<pre><code>...
add $5, $6              # $5 = $5 + $6, i.e. something that doesn&#39;t affect the beq statement
beq $1, $2, some_label  # Branch to some_label if $1 == $2
nop
...
</code></pre>

<p>Ett exempel <em>med</em> delayed branching:</p>

<pre><code>...
beq $1, $2, some_label  # Branch to some_label if $1 == $2
add $5, $6              # $5 = $5 + $6. Now replaces the nop statement
...
</code></pre>

<p><strong>Alternativ förklaring:</strong> Det är att assemblern ordnar om instruktionerna så att något alltid görs direkt efter en 
branch instruktion (i branch delay slot) där processorn annars hade idlat.</p></li>
<li><p>Ge  exempel  på  en  kompilatorteknik  som  kan  användas  för  att  unvika/hantera  konflikter i pipelinen.</p>

<p>Delayed branching är en teknik som kompilatorer kan använda för att minska pipelinekonflikter.
Även automatisk nop-insättning efter alla instruktioner känsliga för strukturella- och datakonflikter, som <code>lw</code> (Load word).</p>

<p>Båda dessa utnyttjas när man anger assemblydirektivet &#39;.setreorder&#39; i MIPS.</p></li>
</ol>

<h2>Minne</h2>

<ol>
<li><p>Hur  lagras  information  på  en  hårddisk?</p>

<p>En hårddisk (ej SSDs) lagrar information på magnetiska skivor som läses med ett läshuvud.</p></li>
<li><p>Vad  är  random  access  när  man  talar  om  minnen?</p>

<p>Att alla sektorer på minnet tar lika ungefär lång tid att hämta.</p></li>
<li><p>Ge  exempel  på  minne  som  inte  har  random  access?</p>

<p>I hårddiskar (ej SSDs) kan olika sektorer ta olika lång tid att hämta beroende på sektorns avstånd från läshuvudet tidigare position. 
Därmed går det snabbare att läsa i sekvens på hårddiskar. 
Defragmentering kan användas för att minimera avståndet mellan relaterade sektorer.</p></li>
<li><p>Vad  är  en  minneshierarki?</p>

<p>En minneshierarki är ett system där man har snabba, men små, minnen högst upp (närmast CPUn) och långsamma, men stora, minnen längst ner (längst ifrån CPUn). </p>

<p>Det finns ofta flera nivåer emellan, ett exempel på en vanlig hierarki är: </p>

<ul>
<li>Register</li>
<li>L1 cache</li>
<li>L2 cache</li>
<li>L3 cache</li>
<li>RAM</li>
<li>Hårddiskar (som i sig själva kan innehålla minneshierarkier då de själva har cacheminne)</li>
<li>Externa lagringsmedia</li>
</ul>

<p>Siffror som </p></li>
<li><p>Varför  uppstår  en  minneshierarki? </p>

<p>En minneshierarki uppstår för att vi kan inte ha både stora och snabba minnen; man måste kompromissa. Därför lägger man mindre, snabbare minnen närmare processorn för att öka snabbheten och minska accesstid och större, långsammare minnen längre från processorn. </p></li>
<li><p>Vad  kallas  principen  som  gör  att  cacheminne  fungerar?  Förklara  principen.  Ge  ett  exempel  på  programkod  där  cacheminne  INTE  ger  någon  vinst.  </p>

<p>Lokalitetsprincipen, gör </p>

<p>Principen är <em>Lokalitet av referenser</em>, varpå det finns två viktiga subprinciper:
    * Temporal lokalitet:
        Om en instruktion/data blivit refererad nu, så är sannlokhuteten stor att samma referens görs inom kort.
    * Rumslokalitet:
        Om instruktion/data blivit refererat nu, så är sannolikheten stor att instruktioner/data vid addresser i närheten kommer användas inom kort.</p></li>
<li><p>Vad  är  en  cachemiss?  Varför  uppkommer  cachemissar?  Hur  hanteras  det?  </p></li>
<li><p>Cacheminnen  kan  ha  olika  mappningar – vilka?  Hur  fungerar  varje  mappning? </p></li>
</ol>

<pre><code>- **Direktmappning**
    Man placerar instruktionen på *nästan* samma plats som den är på i minnet. Man placerar den på följande index:
    `Cacheindex = Instruktionens address i minnet % cachestorlek`
    Är den platsen i cachen upptagen av något annat så skriver man helt enkelt över det.

- **Associativemappning**
    Innebär att man bara fyller på cacheminnet på följd och använder ersättningsalgoritm vid cachemiss i ett fullt cacheminne.

- **Set-associative mappning**
    Detta är en blandning mellan direkt- och associativmappning. Man delar in cacheminnet i flera *sets*, mappar vissa instruktionsaddresser till dessa sets, och använder associativmappning inom varje set. Ett exempel skulle vara om man har två sets, en för alla instruktioner med en udda address och en för alla med jämn address. Inom varje set använder man dock associativmappning.
</code></pre>

<ol>
<li><p>I  direktmappning,  hur  ersätts  cacherader  vid  cachemissar?</p>

<p>De skrivs över utan vidare.</p></li>
<li><p>Vad  är  en  ersättningsalgoritm?    </p>

<p>En algoritm som används vid associativemappning (samt inuti varje set i set-associative mappning) för att bestämma vilken/vilket cacherad/cacheblock som ska kastas ut för att göra plats för det nya. Det finns tre stycken nämnda i cacheminneföreläsningen i denna kursen:</p></li>
</ol>

<pre><code>- **Least recently used (LRU)** – kandidat är den cacherad vilken varit i cachen men som inte blivit refererad (läst/skriven) på länge
- **First-In First Out (FIFO)** – kandidat är den som varit längst i cacheminnet
- **Least frequently used (LFU)** – kandidat är den cacherad som refererats mest sällan 
</code></pre>

<ol>
<li><p>Vad  menas  med  att  cacheminnet  inte  är  konsistent?  Hur  hålls  ett  cacheminne  konsistent?</p></li>
<li><p>Antag  ett  program  som  exekverar  alla  instruktioner  i  en  sekvens  (en  i  taget)  och  att  det  finns  ett  cacheminne  för  instruktioner  där  cacherader  har  storlek  64bytes  och  varje  instruktion  kräver  2  bytes.  Vad  är  sannolikheten  för  att  nästa  instruktion  finns  i  samma cacherad  som  förra  instruktion?</p>

<p>Sannolikheten är <code>31/32</code> då antalet instruktioner per cacherad är <code>64/2 = 32</code> vilket innebär att sannolikheten är <code>1/32</code> att nästa instruktion ligger på nästa cacherad. Det omvända fallet blir då som sagt <code>1-(1/32) = 31/32</code>.</p></li>
<li><p>Vad  är  fördelen  med  paging?</p>

<p>Att man dynamiskt laddar in data i minnet på begäran och laddar ur det när det ej används.</p></li>
<li><p>Vad  är  nackdelar  med  paging?</p>

<p>Det tar tid att ladda in och ur sidor ur minnet.</p></li>
<li><p>Vad  är  fragmentering  när  vi  pratar  om  paging?</p></li>
<li><p>Vad  är  skillnaden  på  extern  fragmentering  och  inter  fragmentering?</p>

<p>Intern fragmentering är när hela kapaciteten av ett block inte används.
Extern fragmentering är när det uppstår icke-allokerade luckor i minnet mellan allokerade segment.</p></li>
<li><p>Vad  är  paging?</p>

<p>Paging är en metod för att ladda in program och data från sekundärminnet till primärminnet.    </p></li>
<li><p>Vad  är  en  sida  (page),  ram  (frame)?</p>

<p>Primärminnet är uppdelat i frames, som kan fyllas med pages som är uppdelade program eller data.</p></li>
<li><p>Om  en  sida  är  2  kBytes,  kan  man  säga  något  om  storleken  på  primärminnet?  Kan  man  säga  något  om  storleken  av  en  ram?</p></li>
<li><p>Vad  är  demand  paging?</p>

<p>Demand paging är när sidor endast laddas när deras innehåll efterfrågas och aldrig innan.</p></li>
<li><p>Vad  är  så  kallad   trashing?  När  uppkommer  det?</p>

<p>Thrashing uppkommer när ett program friar upp ramar som regelbundet används.
Detta gör att de genomgår flera cykler av att laddas in och laddas ur under 
kort tid vilket leder till att en stor mängd resurser går åt att ladda in och ur sidor minnet.</p></li>
<li><p>Vad  är  skillnaden  på  paging  och  virtuellt  minne?</p>

<p>Virtuellt minne mappar en process virtuella adressintervall till sidor i primärminnet eller data i sekundärminnet.
Paging hanterar (ur)laddningen av data från sekundärminnet till primärminnet.</p></li>
<li><p>Vad  är  sidfel?</p>

<p>Sidfel är när ett program försöker komma åt data som inte ligger i RAM minnet.</p></li>
<li><p>Vad  händer  vid  sidfel?  Hur  hanteras  det?</p>

<p>Operativsystemet fångar upp felet, 
letar upp datan i sekundärminnet, 
skaffar sig en tom ram,
om ingen tom ram finns så används en sidersättningsalgoritm för att fria upp en ram,
datan laddas in i ramen som läggs till i sidtabellen, 
instruktionen som gav upphov till felet körs om och
återställer därmed kontrollen till programmet.</p></li>
</ol>

<h2>Operativsystem</h2>

<ol>
<li><p>Vad  gör  ett  operativsystem?</p>

<p>Kärnan hanterar hårdvaruresurser, t.ex. arbetsminne, in/ut-enheter och processortid. Användarprogram </p>

<p><em>Alternativt svar:</em> </p>

<ul>
<li>Filsystemshantering</li>
<li>Minneshantering</li>
<li>Processhantering</li>
<li>Avbrott</li>
<li>In- och utmatning</li>
<li>Nätverk</li>
<li>Säkerhet</li>
<li>Drivrutiner</li>
</ul></li>
<li><p>Vad  är  multitasking?</p>

<p>Att flera program körs samtidigt (eller iallafall att det uppfattas som att de gör det)</p></li>
<li><p>En  användare  känner  att  flera  program  exekverar  samtidigt,  hur  är  det möjligt?  </p>

<p>Schemaläggaren tilldelar klockcykler på ett sådant sätt att användaren inte märker av att saker görs i en viss ordning och inte samtidigt.</p></li>
<li><p>Vad  är  ett  kontextbyte?</p>

<p>Ett byte av vilken tråd som för stunden exekverar.</p></li>
<li><p>Hur  går  ett  kontextbyte  till?  Hur  vet  man  om  att  det  ska  ske?  Vem  är  inblandad?  </p>

<p>Först har schemaläggaren beslutat om ett byte mellan två trådar. När detta byte sedan genomförs så sparas tillståndet (t.ex. register) undan för 
den nuvarande tråden för att den nya tråden ska kunna ladda in det tillstånd som den nya tråden har undansparat sedan tidigare,
detta är för att bevara trådar/processers tillstånd mellan byte av trådar.</p></li>
<li><p>Behövs  avbrott  för  att  klara  av  att  göra  kontextbyten?</p>

<p>Nej, man kan också göra det genom </p></li>
<li><p>Hur  fungerar  avbrott?</p></li>
<li><p>Om  man  skapar  en  struktur  för  att  lagra  filer,  vad  vill  man  uppnå?</p>

<p>Kunna strukturellt allokera utrymme åt filer på disken.</p></li>
<li><p>Om  man  ska  läsa  in  en  fil  från  en  hårddisk,  vad  påverkar  lästiden?</p>

<p>Fragmentering
Filsystemet
Hårddiskens specifikationer</p></li>
</ol>

<hr>

<p>Källan för instuderingsfrågorna finns i <code>instuderingfrågor.pdf</code> som ursprungligen kommer härifrån http://www.eit.lth.se/fileadmin/eit/courses/eit070/2015/Instuderingsfr%E5gor.pdf.</p>

      </div>
    </div>
  </body>
</html>
